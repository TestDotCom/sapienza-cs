#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsart
\use_default_options true
\begin_removed_modules
eqs-within-sections
figs-within-sections
\end_removed_modules
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Cloud computing
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Fundamental concepts and models
\end_layout

\begin_layout Subsection
The cloud
\end_layout

\begin_layout Definition*
(NIST) 
\series bold
Cloud computing
\series default
 is a model for enabling ubiquitous, convenient, on-demand network access
 to a shared pool of configurable computing resources (e.g., networks, servers,
 storage, applications, and services) that can be rapidly provisioned and
 released with minimal management effort or service provider interaction.
\end_layout

\begin_layout Standard
This cloud model is composed of five essential characteristics:
\end_layout

\begin_layout Itemize

\series bold
on-demand self-service
\series default
.
 A consumer can unilaterally provision computing capabilities, such as server
 time and network storage, as needed automatically without requiring human
 interaction with each service provider
\end_layout

\begin_layout Itemize

\series bold
broad network access
\series default
.
 Capabilities are available over the network and accessed through standard
 mechanisms
\end_layout

\begin_layout Itemize

\series bold
resource polling
\series default
.
 The provider computing resources are pooled to serve multiple consumers
 using a 
\series bold
multi-tenant model
\series default
, with different physical and virtual resources dynamically assigned and
 reassigned according to consumer demand.
 Multi-tenancy enables an instance of a program to serve different consumers,
 whereby each is isolated from the other
\end_layout

\begin_layout Itemize

\series bold
rapid elasticity
\series default
.
 Capabilities can be elastically provisioned and released, in some cases
 automatically, to scale rapidly outward and inward commensurate with demand
\end_layout

\begin_layout Itemize

\series bold
measured service
\series default
s.
 Cloud systems automatically control and optimize resource use by leaveraging
 a metering capability at some level of abstraction appropriate to the type
 of service
\end_layout

\begin_layout Standard
Cloud computing is a phenomenon touching on the entire stack: from the underlyin
g hardware to the high-level software services and applications.
 It introduces the concept of 
\begin_inset Quotes eld
\end_inset

everything as a service
\begin_inset Quotes erd
\end_inset

, mostly referred as 
\series bold
XaaS
\series default
.
 Another important aspect of cloud computing is its 
\series bold
utility-oriented
\series default
 approach.
 More than any other trend in distributed computing, cloud computing focuses
 on delivering services with a given pricing model, in most cases a “pay-per-use
” strategy.
 Even though many cloud computing services are freely available for single
 users, enterprise-class services are delivered according a specific pricing
 scheme.
 In this case users subscribe to the service and establish with the service
 provider a Service-Level Agreement (
\series bold
SLA
\series default
), defining the quality-of-service parameters under which the service is
 delivered.
\end_layout

\begin_layout Subsubsection
Services models
\end_layout

\begin_layout Standard
The driving motivation behind cloud computing is to provide IT resources
 as 
\series bold
services
\series default
 that encapsulate other IT resources, while offering functions for clients
 to use and leverage remotely.
 In contrast IT resources hosted in a conventional IT enterprise, within
 an organizational boundary, are considered 
\series bold
on-premise
\series default
.
 An IT resource that is on-premise cannot be cloud-based, and vice-versa.
\end_layout

\begin_layout Itemize
Software as a Service (
\series bold
SaaS
\series default
).
 The capability provided to the consumer is to use the provider's applications
 running on a cloud infrastructure
\end_layout

\begin_layout Itemize
Platform as a Service (
\series bold
PaaS
\series default
).
 The capability provided to the consumer is to deploy onto the cloud infrastruct
ure consumer-created or acquired applications developed using programming
 languages, libraries, services, and tools supported by the provider
\end_layout

\begin_layout Itemize
Infrastructure as a Service (
\series bold
IaaS
\series default
).
 The capability provided to the consumer is to provision processing, storage,
 network, and other foundamental computing resources where the consumer
 is able to deploy and run arbitrary software, which can include OS and
 applications
\end_layout

\begin_layout Subsubsection
Deployment models
\end_layout

\begin_layout Standard
A cloud deployment model represents a specific type of cloud environment,
 primarily distinguished by ownership, size, and access.
\end_layout

\begin_layout Itemize

\series bold
private cloud
\series default
.
 The cloud infrastructure is provisioned for exclusive use by a single organizat
ion comprising multiple consumers
\end_layout

\begin_layout Itemize

\series bold
community cloud
\series default
.
 The cloud infrastructure is provisioned for exclusive use by a specific
 community of consumers from organizations that have shared concerns 
\end_layout

\begin_layout Itemize

\series bold
public cloud
\series default
.
 The cloud infrastructure is provisioned for open use by the general public
\end_layout

\begin_layout Itemize

\series bold
hybrid cloud
\series default
.
 The cloud infrastructure is a composition of two or more distinct cloud
 infrastructures that remain unique entities, but are bound together by
 standardized or proprietary technology that enables data and application
 portability
\end_layout

\begin_layout Subsubsection
Main roles 
\end_layout

\begin_layout Standard
Organizations and humans can assume different types of pre-defined roles
 depending on how they relate to and/or interact with a cloud and its hosted
 IT resources.
\end_layout

\begin_layout Itemize

\series bold
cloud consumer
\series default
.
 Person or organization that maintains a business relationship withm and
 uses services from, cloud service providers
\end_layout

\begin_layout Itemize

\series bold
cloud provider
\series default
.
 Person, organization or entity responsible for making a service available
 to service consumers
\end_layout

\begin_layout Itemize

\series bold
cloud carrier
\series default
.
 The intermediary that provides connectivity and transport of cloud services
 between cloud providers and cloud consumers
\end_layout

\begin_layout Itemize

\series bold
cloud broker
\series default
.
 An entity that manages the use, performance and delivery of cloud services,
 and negotiates relationships between cloud providers and cloud consumers
\end_layout

\begin_layout Itemize

\series bold
cloud auditor
\series default
.
 A party that can conduct independent assessment of cloud services, information
 system operations, performance and security of the cloud implementation
\end_layout

\begin_layout Subsubsection
Business drivers
\end_layout

\begin_layout Standard
The most common economic rationale for investing in cloud-based IT resources
 is in the up-front 
\series bold
cost reduction
\series default
 - or outright elimination - of IT investments, namely hardware and software
 purchases and ownership costs.
 This allows enterprises to start small and accordingly increase IT resource
 allocation as required.
\end_layout

\begin_layout Standard

\series bold
Capacity planning
\series default
 allows cloud users to determe and fulfill future demands of IT resources,
 products, and services.
 Within this context, capacity represents the maximum amount of work that
 an IT resource is capable of delivering in a given period of time.
 A discrepancy between the capacity of an IT resource and its demand can
 result in a system becoming either inefficient (
\series bold
over-provisioning
\series default
) or unable to fulfill user needs (
\series bold
under-provisioning
\series default
).
 Capacity planning is focused on minimizing this discrepancy to achieve
 predictable efficiency and performance.
 Cloud services prefer 
\series bold
lag strategy
\series default
: adding capacity when the IT resource reaches its full capacity.
 Planning for capacity can be challenging because it requires estimating
 usage load fluctuations.
 There is a constant need to balance peak usage requirements without unnecessary
 over-expenditure on infrastructure.
\end_layout

\begin_layout Standard
Finally, businesses need the ability to adapt and evolve to successfully
 face change caused by both internal and external factors.
 
\series bold
Organizational agility
\series default
 is the measure of an organization's responsiveness to change.
 An IT enterprise often needs to respond to business change by scaling its
 IT resources beyond the scope of what was previously predicted or planned
 for.
\end_layout

\begin_layout Subsubsection
Risks and challanges
\end_layout

\begin_layout Standard
Several of the most critical cloud computing challenges pertaining mostly
 to cloud consumers that use IT resources located in public clouds.
\end_layout

\begin_layout Itemize

\series bold
increased security vulnerabilities
\series default
.
 There can be overlapping trust boundaries from different cloud consumers
 due to the fact that cloud-based IT resources are commonly shared.
 Furthermore, cloud providers have a privileged access to cloud consumer
 data.
 This overlapping of trust boundaries and the increased exposure of data
 can provide malicious cloud consumers with greater opportunities to attack
 IT resources
\end_layout

\begin_layout Itemize

\series bold
reduced operational governance control
\series default
.
 Cloud consumers are usually allotted a level of governance control that
 is lower than that over on-premise IT resources.
 This can introduce risks associated with how the cloud provider operates
 its cloud, as well as the external connections that are required for communicat
ion between the cloud and the cloud consumer.
 Legal contracts, when combined with SLAs, technology inspections, and monitorin
g, can mitigate governance risks and issues
\end_layout

\begin_layout Itemize

\series bold
limited portability
\series default
 between cloud providers.
 Due to a lack of established industry standards within the cloud computing
 industry, public clouds are commonly proprietary to various extents.
 For cloud consumers that have custom-built solutions with dependencies
 on these proprietary environments, it can be challenging to move from one
 cloud provider to another
\end_layout

\begin_layout Itemize
multi-regional compliance and 
\series bold
legal issues
\series default
.
 Cloud consumers will often not be aware of the physical location of their
 IT resources and data when hosted by public clouds.
 For some organizations, this can pose serious legal concerns pertaining
 to industry or government regulations that specify data privacy and storage
 policies
\end_layout

\begin_layout Section
Cloud-enabling technology
\end_layout

\begin_layout Standard
Modern-day clouds are underpinned by a set of primary technology components
 that collectively enable key features and characteristics associated with
 contemporary cloud computing.
\end_layout

\begin_layout Subsection
Data center technology
\end_layout

\begin_layout Standard
Grouping IT resources in close proximity with one another, rather than having
 them geographically dispersed, allows for power sharing, higher efficiency
 in shared IT resource usage, and improved accessibility for IT personnel.
 These are the advantages that naturally popularized the data center concept.
 Modern data centers exist as specialized IT infrastructure used to house
 centralized IT resources, such as servers, databases, networking and telecommun
ication devices, and software systems.
\end_layout

\begin_layout Subsubsection
Virtualization 
\end_layout

\begin_layout Standard
It encompasses a collection of solutions allowing the abstraction of some
 of the fundamental elements for computing, such as hardware, storage, and
 networking.
 Virtualization is essentially a technology that allows creation of different
 computing environments.
 These environments are called virtual because they simulate the interface
 that is expected by a guest.
 
\series bold
Hardware virtualization
\series default
 allows the coexistence of different software stacks on top of the same
 hardware.
 These stacks are contained inside 
\series bold
virtual machine instances
\series default
, which operate in isolation from each other.
 Virtualization technologies are also used to replicate runtime environments
 for programs.
 Applications in the case of 
\series bold
process virtual machines
\series default
, instead of being executed by the operating system, are run by a specific
 program called a 
\series bold
virtual machine
\series default
.
 This technique allows isolating the execution of applications and providing
 a finer control on the resource they access.
 Process virtual machines offer a higher level of abstraction with respect
 to hardware virtualization, since the guest is only constituted by an applicati
on rather than a complete software stack.
\end_layout

\begin_layout Subsubsection
Automation
\end_layout

\begin_layout Standard
Autonomic computing refers to the ability of a computer system to self-manage,
 which includes the following capabilities:
\end_layout

\begin_layout Itemize

\series bold
self-configuration
\series default
: ability to accommodate varying and possibly unpredictable conditions 
\end_layout

\begin_layout Itemize

\series bold
self-healing
\series default
: ability to remain functioning when problems arise
\end_layout

\begin_layout Itemize

\series bold
self-protection
\series default
: ability to detect threats and take appropriate actions
\end_layout

\begin_layout Itemize

\series bold
self-optimization
\series default
: constant monitoring for optimal operation
\end_layout

\begin_layout Standard
Autonomic systems are commonly modeled as closed-loop control systems where
 sensors monitor the external conditions and feed the collected data back
 to the decision logic.
 The aim is to have systems that can self-run while adapting to increasing
 system complexity, without the need for any user input.
 These systems can have high levels of built-in artificial intelligence
 that remain hidden from the users.
 Autonomic computing supports several cloud computing characteristics, including
:
\end_layout

\begin_layout Itemize

\series bold
elasticity
\series default
.
 Autonomic systems can monitor usage conditions and leverage cloud-based
 IT resources to automatically acquire and free IT resources as needed for
 the purpose of maintaining required service levels
\end_layout

\begin_layout Itemize

\series bold
resiliency
\series default
.
 Autonomic systems can automatically detect unavailable IT resources and
 self-respond to allocate alternative IT resources as required
\end_layout

\begin_layout Subsubsection
High availability 
\end_layout

\begin_layout Standard
Since any form of data center outage significantly impacts business continuity
 for the organizations that use their services, data centers are designed
 to operate with increasingly higher levels of redundancy to sustain availabilit
y.
 Data centers usually have redundant, uninterruptable power supplies, cabling,
 and environmental control subsystems in anticipation of system failure,
 along with communication links and clustered hardware for load balancing.
\end_layout

\begin_layout Subsection
Web technology
\end_layout

\begin_layout Standard
The web is the primary interface through which cloud computing delivers
 its services.
 At present, the web encompasses a set of technologies and services that
 facilitate interactive information sharing, collaboration, user-centered
 design, and application composition.
 This evolution has transformed the web into a rich platform for application
 development and is known as 
\series bold
web 2.0
\series default
, a new way in which developers architect applications and deliver services
 through the Internet and provides new experience for users of these application
s and services.
 Web 2.0 brings interactivity and flexibility into web pages, providing enhanced
 user experience.
\end_layout

\begin_layout Section
Principles of distributed computing
\end_layout

\begin_layout Standard
As a general definition of the term distributed system, we use the one proposed
 by Tanenbaum et.
 al: 
\begin_inset Quotes eld
\end_inset

A distributed system is a collection of independent computers that appears
 to its users as a single coherent system
\begin_inset Quotes erd
\end_inset

.
 Communication is another fundamental aspect of distributed computing.
 A distributed system is one in which components located at networked computers
 communicate and coordinate their actions only by passing messages.
\end_layout

\begin_layout Standard
A distributed system is the result of the interaction of several components
 that traverse the entire computing stack from hardware to software.
 At the very bottom layer, computer and network hardware constitute the
 physical infrastructure; these components are directly managed by the operating
 system, which provides the basic services for interprocess communication
 (
\series bold
IPC
\series default
), process scheduling and management, and resource management in terms of
 file system and local devices.
 Taken together these two layers become the platform on top of which specialized
 software is deployed to turn a set of networked computers into a distributed
 system.
\end_layout

\begin_layout Standard
The middleware layer leverages such services to build a uniform environment
 for the development and deployment of distributed applications, completely
 independent from the underlying operating system and hiding all the heterogenei
ties of the bottom layers.
 The top of the distributed system stack is represented by the applications
 and services designed and developed to use the middleware.
\end_layout

\begin_layout Subsection
The AKF scale cube
\end_layout

\begin_layout Standard
Imagine a cube drawn with the aid of a three-dimensional axis for a guide.
 The initial point, with coordinates of 
\begin_inset Formula $(0,0,0)$
\end_inset

, is the point of least scalability within any system.
 It consists of a single monolithic solution deployed on a single server.
 It might scale up with larger and faster hardware, but it will not scale
 out.
 As such, it will limit your growth to that which can be served by a single
 unit.
 In other words, your system will be bound by how fast the server runs the
 application in question and how well the application is tuned to use the
 server.
\end_layout

\begin_layout Standard
Making modifications to your solution for the purposes of scaling moves
 you along one of the three axes.
 Equivalent effort applied to any axis will not always return equivalent
 results.
 Choosing one axis does not preclude you from making use of other axes later.
 When we make choices about what to implement, we should select the splits
 that have the highest return (in terms of scale) for our effort and that
 meet our needs in terms of delivering scalability in time for customer
 demand.
 
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $x\text{-axis}$
\end_inset

, or 
\series bold
vertical scaling
\series default
, is very useful and easy to implement: simply clone the activity among
 several participants.
 Scaling along the 
\begin_inset Formula $x\text{-axis}$
\end_inset

 starts to fail, however, when you have a lot of different tasks requiring
 significantly different information from many potential sources.
 Fast transactions start to run at the speed of slow transactions, and everythin
g starts to work suboptimally.
 
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $y\text{-axis}$
\end_inset

, or 
\series bold
horizontal scaling
\series default
, helps to solve that problem by isolating transaction type and speed to
 systems and people specializing in that area of data or service.
 Slower transactions are now bunched together, but because the data set
 has been reduced relative to the 
\begin_inset Formula $x\text{-axis}$
\end_inset

 only example, they run faster than they had previously.
 Fast transactions also speed up because they are no longer competing for
 resources with the slower transactions and their data set has been reduced.
 Monolithic systems are reduced to components that operate more efficiently
 and can scale for data and transaction needs.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $z\text{-axis}$
\end_inset

 not only helps us scale transactions and data, but may also help with monolithi
c system deconstruction.
 Furthermore, we can now move teams and systems around geographically and
 start to gain benefits from this geographic dispersion, such as disaster
 recovery.
 
\end_layout

\begin_layout Subsection
Software architectural styles
\end_layout

\begin_layout Standard
Software architectural styles are based on the logical arrangement of software
 components.
 They are helpful because they provide an intuitive view of the whole system,
 despite its physical deployment.
 They also identify the main abstractions that are used to shape the components
 of the system and the expected interaction patterns between them.
 These models constitute the foundations on top of which distributed systems
 are designed from a logical point of view.
 
\end_layout

\begin_layout Subsubsection
Call-and-return architecture
\end_layout

\begin_layout Standard
This category identifies all systems that are organised into components
 mostly connected together by method calls.
 The activity of systems modeled in this way is characterized by a chain
 of method calls whose overall execution and composition identify the execution
 of one or more operations.
 The internal organization of components and their connections may vary.
\end_layout

\begin_layout Itemize

\series bold
top-down
\series default
 style.
 Systems developed according to this style are composed of one large main
 program that accomplishes its tasks by invoking subprograms or procedures.
 Method calls can also extend beyond the boundary of a single process by
 leveraging techniques such as remote procedure call (
\series bold
RPC
\series default
) and all its descendants.
 This architectural style is quite intuitive from a design point of view
 but hard to maintain and manage in large systems
\end_layout

\begin_layout Itemize

\series bold
layered
\series default
 style.
 Each layer generally operates with at most two layers: the one that provides
 a lower abstraction level and the one that provides a higher abstraction
 layer.
 A user or client generally interacts with the layer at the highest abstraction,
 which, in order to carry its activity, interacts and uses the services
 of the lower layer.
 It is also possible to have the opposite behavior: events and callbacks
 from the lower layers can trigger the activity of the higher layer and
 propagate information up through the stack.
 The advantage is that it supports a modular design of systems and allows
 to decompose the system by encapsulating together all the operations that
 belong to a specific level.
 Layers can be replaced as long as they are compliant with the expected
 protocols and interfaces, thus making the system flexible.
 The main disadvantage is constituted by the lack of extensibility, since
 it is not possible to add layers without changing the protocols and the
 interfaces between layers
\end_layout

\begin_layout Subsubsection
Independent components based
\end_layout

\begin_layout Standard
This class of architectural style models systems in terms of independent
 components that have their own life cycles, which interact with each other
 to perform their activities.
\end_layout

\begin_layout Itemize

\series bold
communicating processes
\series default
.
 Components are represented by independent processes that leverage IPC facilitie
s for coordination management.
 This is an abstraction that is quite suitable to modeling distributed systems
 that, being distributed over a network of computing nodes, are necessarily
 composed of several concurrent processes.
 The conceptual organization of these processes and the way in which the
 communication happens vary according to the specific model used, either
 peer-to-peer or client/server
\end_layout

\begin_layout Itemize

\series bold
event systems
\series default
.
 Components of the system are loosely coupled and connected.
 In addition to exposing operations for data and state manipulation, each
 component also publishes (or announces) a collection of events with which
 other components can register.
 The main advantage of such an architectural style is that it fosters the
 development of open systems: new modules can be added and easily integrated
 into the system as long as they have compliant interfaces for registering
 to the events.
 This architectural style solves some of the limitations observed for the
 top-down and object-oriented styles.
 First, the invocation pattern is implicit, and the connection between the
 caller and the callee is not hard-coded.
 Second, the event source does not need to know the identity of the event
 handler in order to invoke the callback.
 The disadvantage of such a style is that it relinquishes control over system
 computation.
 When a component triggers an event, it does not know how many event handlers
 will be invoked and whether there are any registered handlers.
 This information is available only at runtime and, from a static design
 point of view, becomes more complex to identify the connections among component
s and to reason about the correctness of the interactions
\end_layout

\begin_layout Subsection
System architectural styles
\end_layout

\begin_layout Standard
System architectural styles cover the physical organization of components
 and processes over a distributed infrastructure.
 They provide a set of reference models for the deployment of such systems
 and help engineers not only have a common vocabulary in describing the
 physical layout of systems but also quickly identify the major advantages
 and drawbacks of a given deployment and whether it is applicable for a
 specific class of applications.
\end_layout

\begin_layout Subsubsection
Client/server
\end_layout

\begin_layout Standard
These two components interact with each other through a network connection
 using a given protocol.
 The communication is unidirectional: The client issues a request to the
 server, and after processing the request the server returns a response.
 The client/server model is suitable in many-to-one scenarios, where the
 information and the services of interest can be centralized and accessed
 through a single access point: the server.
 In general, multiple clients are interested in such services and the server
 must be appropriately designed to efficiently serve requests coming from
 different clients.
\end_layout

\begin_layout Subsubsection
Peer-to-peer
\end_layout

\begin_layout Standard
Introduces a symmetric architecture in which all the components, called
 peers, play the same role and incorporate both client and server capabilities
 of the client/server model.
 More precisely, each peer acts as a server when it processes requests from
 other peers and as a client when it issues requests to other peers.
 Therefore, this model is quite suitable for highly decentralized architecture,
 which can scale better along the dimension of the number of peers.
 The disadvantage of this approach is that the management of the implementation
 of algorithms is more complex than in the client/server model.
 
\end_layout

\begin_layout Subsection
Models for interprocess communication: message passing
\end_layout

\begin_layout Standard
Distributed systems are composed of a collection of concurrent processes
 interacting with each other by means of a network connection.
 IPC is what ties together the different components of a distributed system,
 thus making them act as a single system.
 There are several different models in which processes can interact with
 each other; these map to different abstractions for IPC.
 At a lower level, IPC is realized through the fundamental tools of network
 programming, with sockets being the most popular IPC primitive for implementing
 communication channels between distributed processes.
\end_layout

\begin_layout Subsubsection
Remote Procedure Call
\end_layout

\begin_layout Standard
This paradigm extends the concept of procedure call beyond the boundaries
 of a single process, thus triggering the execution of code in remote processes.
 In this case, underlying client/server architecture is implied.
 The server process maintains a registry of all the available procedures
 that can be remotely invoked and listens for requests from clients that
 specify which procedure to invoke, together with the values of the parameters
 required by the procedure.
 RPC maintains the synchronous pattern that is natural in IPC and function
 calls.
 Therefore, the calling process thread remains blocked until the procedure
 on the server process has completed its execution and the result (if any)
 is returned to the client.
 An important aspect of RPC is 
\series bold
marshaling
\series default
, which identifies the process of converting parameter and return values
 into a form that is more suitable to be transported over a network through
 a sequence of bytes.
 The term 
\series bold
unmarshaling
\series default
 refers to the opposite procedure.
\end_layout

\begin_layout Subsubsection
Distributed Object framework
\end_layout

\begin_layout Standard
Extend object-oriented programming systems by allowing objects to be distributed
 across a heterogeneous network and provide facilities so that they can
 coherently act as though they were in the same address space.
 Distributed object frameworks leverage the basic mechanism introduced with
 RPC and extend it to enable the remote invocation of object methods and
 to keep track of references to objects made available through a network
 connection.
 With respect to the RPC model, the infrastructure manages instances that
 are exposed through well-known interfaces instead of procedures.
 Therefore, the common interaction pattern is the following: 
\end_layout

\begin_layout Enumerate
the server process maintains a registry of active objects that are made
 available to other processes
\end_layout

\begin_layout Enumerate
the client process, by using a given addressing scheme, obtains a reference
 to the active remote object
\end_layout

\begin_layout Enumerate
the client process invokes the methods on the active object by calling them
 through the reference previously obtained.
 Parameters and return values are marshaled as happens in the case of RPC
\end_layout

\begin_layout Standard
Distributed object frameworks introduce objects as first-class entities
 for IPC.
 They are the principal gateway for invoking remote methods but can also
 be passed as parameters and return values.
 This poses an interesting problem, since object instances are complex instances
 that encapsulate a state and might be referenced by other components.
 Passing an object as a parameter or return value involves the duplication
 of the instance on the other execution context.
 This operation leads to two separate objects whose state evolves independently.
 The duplication becomes necessary since the instance needs to trespass
 the boundaries of the process.
\end_layout

\begin_layout Standard
An alternative to this standard process, which is called marshaling by value,
 is 
\series bold
marshaling by reference
\series default
.
 In this second case the object instance is not duplicated and a proxy of
 it is created on the server side (for parameters) or the client side (for
 return values).
 Marshaling by reference is a more complex technique and generally puts
 more burden on the runtime infrastructure since remote references have
 to be tracked.
 Being more complex and resource demanding, marshaling by reference should
 be used only when duplication of parameters and return values lead to unexpecte
d and inconsistent behavior of the system.
 
\end_layout

\begin_layout Subsubsection
Service-oriented computing
\end_layout

\begin_layout Standard
Organizes distributed systems in terms of services, which represent the
 major abstraction for building systems.
 Service orientation expresses applications and software systems as aggregations
 of services that are coordinated within a service-oriented architecture
 (
\series bold
SOA
\series default
).
 Even though there is no designed technology for the development of service-orie
nted software systems, Web services are the de facto approach for developing
 SOA.
\end_layout

\begin_layout Standard
A 
\series bold
service
\series default
 encapsulates a software component that provides a set of coherent and related
 functionalities that can be reused and integrated into bigger and more
 complex applications.
 The term service is a general abstraction that encompasses several different
 implementations using different technologies and protocols, usually identified
 by four major characteristics: boundaries are explicit, services are autonomous
, services share schema and contracts (not class or interface definitions),
 services compatibility is based on policy.
\end_layout

\begin_layout Subsection
Service-Oriented Architecture
\end_layout

\begin_layout Standard

\series bold
SOA
\series default
 organizes a software system into a colletion of interacting services.
 SOA encompasses a set of design principles that structure system development
 and provide means for integrating components into a coherent and decentralized
 system.
 SOA-based computing packages functionalities into a set of interoperable
 services, which can be integrated into different software systems belonging
 to separate business domains.
 
\end_layout

\begin_layout Standard
Services might aggregate information and data retrieved from other services
 or create workflows of services to satisfy the request of a given service
 consumer.
 This practice is known as 
\series bold
service orchestration
\series default
, representing a single centralized executable business process (the orchestrato
r) that coordinates the interaction among different services.
 The orchestrator is responsible for invoking and combining the services.
 On the other hand, 
\series bold
service choreography
\series default
 coordinates interaction of services in a distributed manner.
 This means that a choreography differs from an orchestration with respect
 to where the logic that controls the interactions between the services
 involved should reside.
\end_layout

\begin_layout Standard
SOA provides a reference model for architecting several software systems,
 especially enterprise business applications and systems.
 In this context, interoperability, standards, and service contracts play
 a fundamental role.
 In particular, recommended guiding principles are standardized service
 contracts, loose coupling, abstractions, reusability, autonomy, lack of
 state, discoverability, and composability.
\end_layout

\begin_layout Subsubsection
Web services
\end_layout

\begin_layout Standard
They leverage Internet technologies and standards for building distributed
 systems.
 Several aspects make Web services the technology of choice for SOA.
 First, they allow for interoperability across different platforms and programmi
ng languages.
 Second, they are based on well-known and vendor-independent standards such
 as HTTP, XML and JSON.
 Third, they provide an intuitive and simple way to connect heterogeneous
 software systems, enabling the quick composition of services in a distributed
 environment.
 Finally, they define facilities for enabling service discovery, which allows
 system architects to more efficiently compose SOA applications, and service
 metering to assess whether a specific service complies with the contract
 between the service provider and the service consumer.
 
\end_layout

\begin_layout Itemize

\series bold
SOAP
\series default
 structures the interaction in terms of messages that are XML documents,
 and leverages the transport level, most commonly HTTP, for IPC.
 These messages can be used for method invocation and result retrieval.
 SOAP has often been considered quite inefficient because of the excessive
 use of markup that XML imposes for organizing the information into a well-forme
d document
\end_layout

\begin_layout Itemize

\series bold
REST
\series default
 provides a model for designing network-based software systems utilizing
 the client/server model and leverages the facilities provided by HTTP for
 IPC without additional burden.
 In a 
\series bold
RESTful
\series default
 system, a client sends a request over HTTP using the standard HTTP methods
 and the server issues a response that includes the representation of the
 resource.
 By relying on this minimal support, it is possible to provide whatever
 it needed to replace the basic and most important functionality provided
 by SOAP, which is method invocation.
 Together with an appropriate URI organization to identify resources, all
 the atomic operations required by a Web service are implemented
\end_layout

\begin_layout Subsection
Microservices
\end_layout

\begin_layout Standard
Initially, a monolithic architecture offers several benefits, since it is
 simple to developt and easy to make radical changes, straightforward to
 test and deploy, easy to scale.
 But over time, as complexity increases, development, testing, deployment,
 and scaling becomes much more difficult.
 Instead, a 
\series bold
microservice
\series default
 is a cohesive, independent process interacting via messages, and a 
\series bold
microservice architecture
\series default
 is a distributed application where all its modules are microservices.
 Microservices manage growing complexity by functionally decomposing large
 systems into a set of independent services.
 By making services completely independent in development and deployment,
 microservices emphasise loose coupling and high cohesion by taking modularity
 to the next level.
 This approach delivers all sorts of benefits in terms of maintainability,
 scalability and so on:
\end_layout

\begin_layout Itemize
services are small and easily maintained, and can be independently deployable
 and scalable
\end_layout

\begin_layout Itemize
continuos integration.
 It is possible to plan gradual transitions to new versions of a microservice
\end_layout

\begin_layout Itemize
microservices naturally lend themselves to containerisation
\end_layout

\begin_layout Itemize
the microservice architecture enables teams to be autonomous
\end_layout

\begin_layout Itemize
better fault isolation
\end_layout

\begin_layout Standard
Like SOA, microservices architectures are made up of loosely coupled, reusable,
 and specialized components that often work independently of one another.
 Microservices also use a high degree of cohesion, otherwise known as bounded
 context.
 Bounded context refers to the relationship between a component and its
 data as a standalone entity or unit with very few dependencies.
 However, we can point out some key differences:
\end_layout

\begin_layout Itemize

\series bold
communication
\series default
 and interoperability.
 In a microservices architecture, each service is developed independently,
 with its own communication protocol, using lightweight and open source
 messaging protocols like HTTP/REST and gRPC.
 On the other hand, each service in SOA must share a common Enterprise Service
 Bus (ESB) middleware, which can be a single point of failure.
 Furthermore, SOA tends to use more heavyweight technologies such as SOAP
 and other WS standards
\end_layout

\begin_layout Itemize

\series bold
data handling
\series default
.
 SOA architecture typically includes a single data storage layer shared
 by all services within a given application, whereas microservices will
 dedicate a server or database for data storage for any service that needs
 it
\end_layout

\begin_layout Itemize

\series bold
service size
\series default
.
 Microservices architectures are made up of highly specialized services,
 each of which is designed to do one thing very well.
 The services that make up SOAs, on the other hand, can range from small,
 specialized services to enterprise-wide services
\end_layout

\begin_layout Standard
Microservices also comes with a bundle of problems that are inherited from
 distributed systems and from SOA:
\end_layout

\begin_layout Itemize
deciding when to adopt the microservice architecture, and finding the right
 set of services, can be challenging
\end_layout

\begin_layout Itemize
distributed systems are complex and so development, testing and deployment
 can be problematic
\end_layout

\begin_layout Itemize
deploying features that span multiple services requires careful coordination
 (among development teams)
\end_layout

\begin_layout Section
Virtualization
\end_layout

\begin_layout Standard

\series bold
Virtualization
\series default
 is a large umbrella of technologies and concepts that are meant to provide
 an abstract environment - whether virtual hardware or an operating system
 - to run applications.
 Virtualization technologies provide a virtual environment for not only
 executing applications but also for storage, memory, and networking.
 Virtualization technologies have gained renewed interested recently due
 to the confluence of several phenomena: underutilization of hardware and
 software resources, lack of space, greening initiatives, and rise of administra
tive costs.
\end_layout

\begin_layout Subsection
Characteristics of virtual environments
\end_layout

\begin_layout Standard
In a virtualized environment there are three major components: guest, host,
 and virtualization layer.
 The 
\series bold
guest
\series default
 represents the system component that interacts with the virtualization
 layer rather than with the host, as would normally happen.
 The 
\series bold
host
\series default
 represents the original environment where the guest is supposed to be managed.
 The 
\series bold
virtualization layer
\series default
 is responsible for recreating the same or a different environment where
 the guest will operate.
 In the case of hardware virtualization, the guest is represented by a system
 image comprising an operating system and installed applications.
 These are installed on top of virtual hardware that is controlled and managed
 by the virtualization layer, also called the 
\series bold
virtual machine manager
\series default
.
 The host is instead represented by the physical hardware, and in some cases
 the operating system, that defines the environment where the virtual machine
 manager is running.
 The technologies of today allow profitable use of virtualization and make
 it possible to fully exploit the advantages that come with it, such as:
\end_layout

\begin_layout Itemize
increased security.
 All the operations of the guest are generally performed against the virtual
 machine, which then translates and applies them to the host.
 This level of indirection allows the virtual machine manager to control
 and filter the activity of the guest, thus preventing some harmful operations
 from being performed.
 This is a requirement when dealing with untrusted code
\end_layout

\begin_layout Itemize
managed execution.
 Virtualization of the execution environment not only allows increased security,
 but a wider range of features also can be implemented, such as sharing,
 aggregation, emulation, and isolation
\end_layout

\begin_layout Itemize
portability.
 In the case of a hardware virtualization solution, the guest is packaged
 into a virtual image that, in most cases, can be safely moved and executed
 on top of different virtual machines.
 In the case of programming-level virtualization, as implemented by the
 JVM or the .NET runtime, the binary code representing application components
 can be run without any recompilation on any implementation of the corresponding
 virtual machine
\end_layout

\begin_layout Subsubsection
Virtualization and cloud computing
\end_layout

\begin_layout Standard
Besides being an enabler for computation on demand, virtualization also
 gives the opportunity to design more efficient computing systems by means
 of 
\series bold
consolidation
\series default
, which is performed transparently to cloud computing service users.
 Since virtualization allows us to create isolated and controllable environments
, it is possible to serve these environments with the same resource without
 them interfering with each other.
 If the underlying resources are capable enough, there will be no evidence
 of such sharing.
 This practice is also known as server consolidation, while the movement
 of virtual machine instances is called 
\series bold
virtual machine migration
\series default
.
 Because virtual machine instances are controllable environments, consolidation
 can be applied with a minimum impact, either by temporarily stopping its
 execution and moving its data to the new resources or by performing a finer
 control and moving the instance while it is running.
 This second techniques is known as 
\series bold
live migration
\series default
 and in general is more complex to implement but more efficient since there
 is no disruption of the activity of the virtual machine instance.
\end_layout

\begin_layout Enumerate

\series bold
pre-migration
\series default
.
 Determine the migrating VM and the destination host.
 This can be performed manually, but in most circumstances it is automatically
 started by strateges such as load balancing and server consolidation
\end_layout

\begin_layout Enumerate

\series bold
reservation
\series default
 and 
\series bold
iterative pre-copy
\series default
.
 The whole execution state of the VM is stored in memory and sent to the
 destination node, ensuring continuity of service.
 Dirty memory is copied iteratively until its last portion is small enough
 to be handled
\end_layout

\begin_layout Enumerate

\series bold
stop and copy
\series default
.
 The VM is suspended and its apps no longer run (downtime).
 Other non-memory data such as CPU and network states should be sent as
 well 
\end_layout

\begin_layout Enumerate

\series bold
commitment
\series default
.
 the VM reloads its state and recovers the execution of its programs.
 The services provided by this VM continues
\end_layout

\begin_layout Enumerate

\series bold
activation
\series default
.
 The network connection is redirected to the new VM and the dependency to
 the source host is cleared, removing the original VM from the source host
\end_layout

\begin_layout Subsection
Execution virtualization
\end_layout

\begin_layout Standard
Execution virtualization includes all techniques that aim to emulate an
 execution environment that is separate from the one hosting the virtualization
 layer.
 Virtualizing an execution environment at different levels of the computing
 stack requires a 
\series bold
reference model
\series default
 that defines the interfaces between the levels of abstractions, which hide
 implementation details.
\end_layout

\begin_layout Standard
Furthermore, the instruction set exposed by the hardware has been divided
 into different security classes that define who can operate with them.
 The first distinction can be made between privileged and nonprivileged
 instructions.
 
\series bold
Nonprivileged instructions
\series default
 are those instructions that can be used without interfering with other
 tasks because they do not access shared resources.
 This category contains e.g.
 all the floating, fixed-point, and arithmetic instructions.
 
\series bold
Privileged instructions
\series default
 are those that are executed under specific restrictions and are mostly
 used for sensitive operations, which expose or modify the privileged state.
\end_layout

\begin_layout Standard
Some types of architecture feature more than one class of privileged instruction
s and implement a finer control of how these instructions can be accessed.
 For instance, a possible implementation features a hierarchy of privileges
 in the form of ring-based security.
 
\series bold
Ring 0
\series default
 is in the most privileged level and 
\series bold
Ring 3
\series default
 in the least privileged level.
 Ring 0 is used by the kernel of the OS, rings 1 and 2 are used by the OS-level
 services, and Ring 3 is used by the user.
\end_layout

\begin_layout Standard
All the current systems support at least two different execution modes:
 
\series bold
supervisor mode
\series default
 and 
\series bold
user mode
\series default
.
 The first mode denotes an execution mode in which all the instructions
 (privileged and nonprivileged) can be executed without any restriction.
 This mode, also called kernel mode, is generally used by the operating
 system (or the hypervisor) to perform sensitive operations on hardware-level
 resources.
 In user mode, there are restrictions to control the machine-level resources.
 If code running in user mode invokes the privileged instructions, hardware
 interrupts occur and trap the potentially harmful execution of the instruction.
 Despite this, there might be some instructions that can be invoked as privilege
d instructions under some conditions and as nonprivileged instructions under
 other conditions.
 Conceptually, the 
\series bold
hypervisor
\series default
 runs above the supervisor mode; in reality, hypervisors are run in supervisor
 mode, and the division between privileged and nonprivileged instructions
 has posed challenges in designing virtual machine managers.
 It is expected that all the sensitive instructions will be executed in
 privileged mode, which requires supervisor mode in order to avoid traps.
 Without this assumption it is impossible to fully emulate and manage the
 status of the CPU for guest operating systems.
 More recent implementations of ISA
\begin_inset Foot
status open

\begin_layout Plain Layout
Instruction Set Architecture.
\end_layout

\end_inset

 (Intel-VTx and AMD-V) have solved this problem by redesigning such sensitive
 instructions as privileged ones.
 
\end_layout

\begin_layout Subsection
Hardware-level virtualization
\end_layout

\begin_layout Standard
Hardware-level virtualization is a virtualization technique that provides
 an abstract execution environment in terms of computer hardware on top
 of which a guest operating system can be run.
 In this model, the guest is represented by the operating system, the host
 by the physical computer hardware, the virtual machine by its emulation,
 and the virtual machine manager by the hypervisor.
 The hypervisor is generally a program or a combination of software and
 hardware that allows the abstraction of the underlying physical hardware.
 
\end_layout

\begin_layout Subsubsection
Hypervisors
\end_layout

\begin_layout Standard
A fundamental element of hardware virtualization is the hypervisor, or virtual
 machine manager (
\series bold
VMM
\series default
).
 It recreates a hardware environment in which guest operating systems are
 installed.
 There are two major types of hypervisor:
\end_layout

\begin_layout Itemize

\series bold
type I
\series default
 hypervisors run directly on top of the hardware.
 Therefore, they take the place of the operating systems and interact directly
 with the ISA interface exposed by the underlying hardware, and they emulate
 this interface in order to allow the management of guest operating systems.
 This type of hypervisor is also called a native virtual machine since it
 runs natively on hardware
\end_layout

\begin_layout Itemize

\series bold
type II
\series default
 hypervisors require the support of an operating system to provide virtualizatio
n services.
 This means that they are programs managed by the operating system, which
 interact with it through the ABI
\begin_inset Foot
status open

\begin_layout Plain Layout
Application Binary Interface.
\end_layout

\end_inset

 and emulate the ISA of virtual hardware for guest operating systems.
 This type of hypervisor is also called a hosted virtual machine since it
 is hosted within an operating system
\end_layout

\begin_layout Standard
Hardware-level virtualization includes several strategies that differentiate
 from each other in terms of which kind of support is expected from the
 underlying hardware, what is actually abstracted from the host, and whether
 the guest should be modified or not.
 
\end_layout

\begin_layout Itemize

\series bold
hardware-assisted virtualization
\series default
.
 This term refers to a scenario in which the hardware provides architectural
 support for building a virtual machine manager able to run a guest operating
 system in complete isolation.
 At present, examples of hardware-assisted virtualization are the extensions
 to the x86-64 bit architecture introduced with Intel-VTx and AMD-V, which
 are meant to reduce the performance penalties experienced by emulating
 x86 hardware with hypervisors.
 Today there exist many hardware-assisted solutions like Kernel-based Virtual
 Machine (KVM), VirtualBox, Xen, VMware and Hyper-V
\end_layout

\begin_layout Itemize

\series bold
full virtualization
\series default
.
 This refers to the ability to run a program, most likely an operating system,
 directly on top of a virtual machine and without any modification, as though
 it were run on the raw hardware.
 To make this possible, virtual machine managers are required to provide
 a complete emulation of the entire underlying hardware.
 The principal advantage of full virtualization is complete isolation, which
 leads to enhanced security, ease of emulation of different architectures,
 and coexistence of different systems on the same platform
\end_layout

\begin_layout Itemize

\series bold
paravirtualization
\series default
.
 This is a not-transparent virtualization solution that allows implementing
 thin virtual machine managers.
 Paravirtualization techniques expose a software interface to the virtual
 machine that is slightly modified from the host and, as a consequence,
 guests need to be modified.
 The aim of paravirtualization is to provide the capability to demand the
 execution of performance-critical operations directly on the host, thus
 preventing performance losses that would otherwise be experienced in managed
 execution.
 This technique has been successfully used by Xen for providing virtualization
 solutions for Linux-based operating systems specifically ported to run
 on Xen hypervisors
\end_layout

\begin_layout Itemize

\series bold
partial virtualization
\series default
.
 This provides a partial emulation of the underlying hardware, thus not
 allowing the complete execution of the guest operating system in complete
 isolation.
 Partial virtualization allows many applications to run transparently, but
 not all the features of the operating system can be supported, as happens
 with full virtualization.
 An example of partial virtualization is address space virtualization used
 in time-sharing systems; this allows multiple applications and users to
 run concurrently in a separate memory space, but they still share the same
 hardware resources
\end_layout

\begin_layout Standard
Multiple instances of a variety of operating systems may share the virtualized
 hardware resources.
 This contrasts with operating-system-level virtualization, where all instances
 must share a single kernel, though the guest operating systems can differ
 in user space.
\end_layout

\begin_layout Subsubsection
Operating system-level virtualization 
\end_layout

\begin_layout Standard
This offers the opportunity to create different and separated execution
 environments for applications that are managed concurrently.
 Differently from hardware virtualization, there is no virtual machine manager
 or hypervisor, and the virtualization is done within a single operating
 system, where the OS kernel allows for multiple isolated user space instances.
 The kernel is also responsible for sharing the system resources among instances
 and for limiting the impact of instances on each other.
 A user space instance in general contains a proper view of the file system,
 which is completely isolated, and separate IP addresses, software configuration
s, and access to devices.
 
\end_layout

\begin_layout Standard
This virtualization technique can be considered an evolution of the chroot
 mechanism in Unix systems.
 Because Unix systems also expose devices as parts of the file system, by
 using this method it is possible to completely isolate a set of processes.
 Following the same principle, operating system-level virtualization aims
 to provide separated and multiple execution containers for running applications.
 
\end_layout

\begin_layout Standard
Compared to hardware virtualization, this strategy imposes little or no
 overhead because applications directly use OS system calls and there is
 no need for emulation.
 There is no need to modify applications to run them, nor to modify any
 specific hardware, as in the case of hardware-assisted virtualization.
 On the other hand, operating system-level virtualization does not expose
 the same flexibility of hardware virtualization, since all the user space
 instances must share the same operating system.
 This technique is an efficient solution for server consolidation scenarios
 in which multiple application servers share the same technology.
\end_layout

\begin_layout Subsection
Application-level virtualization
\end_layout

\begin_layout Standard
Application-level virtualization is a technique allowing applications to
 be run in runtime environments that do not natively support all the features
 required by such applications.
 In this scenario, applications are not installed in the expected runtime
 environment but are run as though they were.
 In general, these techniques are mostly concerned with partial file systems,
 libraries, and operating system component emulation.
 Such emulation is performed by a thin layer - a program or an operating
 system component - that is in charge of executing the application.
 Emulation can also be used to execute program binaries compiled for different
 hardware architectures.
 In this case, one of the following strategies can be implemented: 
\end_layout

\begin_layout Itemize

\series bold
interpretation
\series default
.
 In this technique every source instruction is interpreted by an emulator
 for executing native ISA instructions, leading to poor performance.
 Interpretation has a minimal startup cost but a huge overhead, since each
 instruction is emulated
\end_layout

\begin_layout Itemize

\series bold
binary translation
\series default
.
 In this technique every source instruction is converted to native instructions
 with equivalent functions.
 After a block of instructions is translated, it is cached and reused.
 Binary translation has a large initial overhead cost, but over time it
 is subject to better performance, since previously translated instruction
 blocks are directly executed
\end_layout

\begin_layout Standard
Emulation, as described, is different from hardware-level virtualization.
 The former simply allows the execution of a program compiled against a
 different hardware, whereas the latter emulates a complete hardware environment
 where an entire operating system can be installed.
 Application virtualization is a good solution in the case of missing libraries
 in the host operating system; in this case a replacement library can be
 linked with the application, or library calls can be remapped to existing
 functions available in the host system.
 Another advantage is that in this case the virtual machine manager is much
 lighter since it provides a partial emulation of the runtime environment
 compared to hardware virtualization.
\end_layout

\begin_layout Section
Process virtualization with Docker
\end_layout

\begin_layout Standard

\series bold
Docker
\series default
 provides the ability to package and run an application in a loosely isolated
 environment called a 
\series bold
container
\series default
.
 Unlike virtual machines, Docker containers do not use any hardware virtualizati
on.
 Instead, it builds on top of the container technology already provided
 by the operating system kernel.
 Programs running inside Docker containers interface directly with the host's
 Linux kernel.
 Many programs can run in isolation without running redundant operating
 systems or suffering the delay of full boot sequences.
\end_layout

\begin_layout Standard
Docker uses a client-server architecture.
 The Docker client talks to the Docker daemon, which does the heavy lifting
 of building, running, and distributing your Docker containers.
 The Docker client and daemon can run on the same system, or you can connect
 a Docker client to a remote Docker daemon.
 Another Docker client is Docker Compose, that lets you work with applications
 consisting of a set of containers.
\end_layout

\begin_layout Subsection
Underlying technology
\end_layout

\begin_layout Subsubsection
Namespaces
\end_layout

\begin_layout Standard
Every running program - or process - on a Linux machine has a unique number
 called a process identifier (PID).
 A PID namespace is a set of unique numbers that identify processes.
 Linux provides tools to create multiple PID namespaces.
 Each namespace has a complete set of possible PIDs.
 Most programs will not need access to other running processes or be able
 to list the other running processes on the system.
 And so Docker creates a new PID namespace for each container by default.
 A container PID namespace isolates processes in that container from processes
 in other containers.
 
\end_layout

\begin_layout Standard
Without a PID namespace, the processes running inside a container would
 share the same ID space as those in other containers or on the host.
 A process in a container would be able to determine what other processes
 were running on the host machine.
 Worse, processes in one container might be able to control processes in
 other containers.
 A process that cannot reference any processes outside its namespace is
 limited in its ability to perform targeted attacks.
\end_layout

\begin_layout Subsubsection
Control groups
\end_layout

\begin_layout Standard
Physical system resources such as memory and time on the CPU are scarce.
 If the resource consumption of processes on a computer exceeds the available
 physical resources, the processes will experience performance issues and
 may stop running.
 Part of building a system that creates strong isolation includes providing
 resource allowances on individual containers.
 Docker Engine on Linux relies on control groups (
\series bold
cgroups
\series default
) to organize processes into hierarchical groups, whose usage of various
 types of resources can then be limited and monitored.
\end_layout

\begin_layout Subsubsection
Images and layers
\end_layout

\begin_layout Standard
Most of the time what we have been calling an image is actually a collection
 of image layers.
 A layer is set of files and file metadata that is packaged and distributed
 as an atomic unit.
 Images maintain parent/child relationships.
 The files available to a container are the union of all layers in the lineage
 of the image that the container was created from.
 Images can have relationships with any other image, including images in
 different repositories with different owners.
\end_layout

\begin_layout Subsubsection
Union Filesystem
\end_layout

\begin_layout Standard
Programs running inside containers know nothing about image layers.
 From inside a container, the filesystem operates as though it's not running
 in a container or operating on an image.
 From the perspective of the container, it has exclusive copies of the files
 provided by the image.
 This is made possible with something called a union filesystem (
\series bold
UFS
\series default
).
 
\end_layout

\begin_layout Standard
Docker uses a variety of union filesystems and can select the best fit for
 the system.
 The filesystem is used to create mount points on the host filesystem that
 abstract the use of layers.
 The layers created are bundled into Docker image layers.
 Likewise, when a Docker image is installed, its layers are unpacked and
 appropriately configured for use by the specific filesystem provider chosen
 for the system.
 Lastly, chroot is used to make the root of the image filesystem the root
 in the container's context.
 This prevents anything running inside the container from referencing any
 other part of the host filesystem.
 
\end_layout

\begin_layout Subsubsection
Container format
\end_layout

\begin_layout Standard
Docker Engine combines the namespaces, control groups, and UnionFS into
 a wrapper called a 
\series bold
container format
\series default
.
 The default container format is libcontainer.
\end_layout

\begin_layout Subsection
Storage
\end_layout

\begin_layout Standard
By default all files created inside a container are stored on a writable
 container layer.
 This means that:
\end_layout

\begin_layout Itemize
the data does not persist when that container no longer exists, and it can
 be difficult to get the data out of the container if another process needs
 it
\end_layout

\begin_layout Itemize
a container writable layer is tightly coupled to the host machine where
 the container is running.
 You can not easily move the data somewhere else
\end_layout

\begin_layout Itemize
writing into a container writable layer requires a storage driver to manage
 the filesystem.
 The storage driver provides a union filesystem, using the Linux kernel.
 This extra abstraction reduces performance as compared to using data volumes,
 which write directly to the host filesystem
\end_layout

\begin_layout Standard
Docker has two options for containers to store files in the host machine,
 so that the files are persisted even after the container stops: volumes,
 and bind mounts.
 If running Docker on Linux you can also use a tmpfs mount.
 If running Docker on Windows you can also use a named pipe.
\end_layout

\begin_layout Subsubsection
Volumes
\end_layout

\begin_layout Standard

\series bold
Volumes
\series default
 are the preferred way to persist data in Docker containers and services.
 Some use cases for volumes include:
\end_layout

\begin_layout Itemize
sharing data among multiple running containers.
 Volumes persist after a container is removed/stopped, and multiple containers
 can mount the same volume simultaneously
\end_layout

\begin_layout Itemize
when the Docker host is not guaranteed to have a given directory or file
 structure.
 Volumes help you decouple the configuration of the Docker host from the
 container runtime
\end_layout

\begin_layout Itemize
store your container data on a remote host or a cloud provider
\end_layout

\begin_layout Itemize
back up, restore, or migrate data from one Docker host to another
\end_layout

\begin_layout Itemize
when an app requires high-performance I/O.
 Volumes are stored in the Linux VM rather than the host, which means that
 the reads and writes have much lower latency and higher throughput
\end_layout

\begin_layout Itemize
when an app requires fully native file system behavior, e.g.
 DBA requires precise control over disk flushing to guarantee transaction
 durability
\end_layout

\begin_layout Subsubsection
Bind mounts
\end_layout

\begin_layout Standard
In general, you should use volumes where possible.
 
\series bold
Bind mounts
\series default
 are appropriate for the following types of use case:
\end_layout

\begin_layout Itemize
sharing configuration files from the host machine to containers.
 This is how Docker provides DNS resolution to containers by default
\end_layout

\begin_layout Itemize
sharing source code or build artifacts between a development environment
 on the Docker host and a container
\end_layout

\begin_layout Itemize
if you use Docker for development this way, your production Dockerfile would
 copy the production-ready artifacts directly into the image, rather than
 relying on a bind mount
\end_layout

\begin_layout Itemize
when the file or directory structure of the Docker host is guaranteed to
 be consistent with the bind mounts the containers require
\end_layout

\begin_layout Subsubsection
Tmpfs mounts
\end_layout

\begin_layout Standard

\series bold
Tmpfs mounts
\series default
 are best used for cases when you do not want the data to persist either
 on the host machine or within the container.
 This may be for security reasons or to protect the performance of the container
 when your application needs to write a large volume of non-persistent state
 data.
\end_layout

\begin_layout Subsection
Networking
\end_layout

\begin_layout Standard
Docker networking subsystem is pluggable, using drivers.
 Several drivers exist by default, and provide core networking functionality:
\end_layout

\begin_layout Itemize

\series bold
bridge
\series default
 its the default network driver.
 Bridge networks are useful when you need multiple containers to communicate
 on the same Docker host
\end_layout

\begin_layout Itemize

\series bold
host
\series default
 is for standalone containers, removes network isolation between the container
 and the Docker host, and use the host networking directly
\end_layout

\begin_layout Itemize

\series bold
overlay
\series default
 networks connect multiple Docker daemons together and enable swarm services
 to communicate with each other
\end_layout

\begin_layout Itemize

\series bold
macvlan
\series default
 networks allow you to assign a MAC address to a container, making it appear
 as a physical device on your network.
 Using the macvlan driver is sometimes the best choice when dealing with
 legacy applications that expect to be directly connected to the physical
 network, rather than routed through the Docker host’s network stack, or
 when you are migrating from a VM setup
\end_layout

\begin_layout Subsection
Higher-level abstractions and orchestration 
\end_layout

\begin_layout Standard
A 
\series bold
swarm
\series default
 consists of multiple Docker hosts which run in swarm mode and act as managers
 and/or workers, which run swarm services.
 Any processes, functionality, or data that must be discoverable and available
 over a network is called a service.
 A service is defined by its optimal state (number of replicas, network
 and storage resources available, exposed ports, and more).
 Docker works to maintain that desired state.
\end_layout

\begin_layout Section
Scaling mechanisms
\end_layout

\begin_layout Subsection
Autoscaling
\end_layout

\begin_layout Standard
Dynamic allocation enables variable utilization as dictated by usage demand
 fluctuations, since unnecessary IT resources are efficiently reclaimed
 without requiring manual interaction.
 The automated scaling listener is configured with workload thresholds that
 dictate when new IT resources need to be added to the workload processing.
 This mechanism can be provided with logic that determines how many additional
 IT resources can be dynamically provided, based on the terms of a given
 cloud consumer provisioning contract.
 The following types of dynamic scaling are commonly used:
\end_layout

\begin_layout Itemize

\series bold
dynamic horizontal scaling
\series default
.
 IT resource instances are scaled out and in to handle fluctuating workloads.
 The automatic scaling listener monitors requests and signals resource replicati
on to initiate IT resource duplication, as per requirements and permissions
\end_layout

\begin_layout Itemize

\series bold
dynamic vertical scaling
\series default
.
 IT resource instances are scaled up and down when there is a need to adjust
 the processing capacity of a single IT resource
\end_layout

\begin_layout Itemize

\series bold
dynamic relocation
\series default
.
 IT resource is relocated to a host with more capacity
\end_layout

\begin_layout Standard
The dynamic scalability architecture can be applied to a range of IT resources,
 including virtual servers and cloud storage devices.
 Besides the core automated scaling listener and resource replication mechanisms
, the following mechanisms can also be used in this form of cloud architecture:
\end_layout

\begin_layout Itemize

\series bold
cloud usage monitor
\series default
.
 Specialized cloud usage monitors can track runtime usage in response to
 dynamic fluctuations caused by this architecture
\end_layout

\begin_layout Itemize
hypervisor.
 The hypervisor is invoked by a dynamic scalability system to create or
 remove virtual server instances, or to be scaled itself
\end_layout

\begin_layout Itemize

\series bold
pay-per-use monitor
\series default
.
 The pay-per-use monitor is engaged to collect usage cost information in
 response to the scaling of IT resources
\end_layout

\begin_layout Subsubsection
Automated scaling listener
\end_layout

\begin_layout Standard
The automated scaling listener mechanism is a service agent that monitors
 and tracks communications between cloud service consumers and cloud services
 for dynamic scaling purposes.
 Workloads can be determined by the volume of cloud consumer-generated requests
 or via backend processing demands triggered by certain types of requests.
 Automated scaling listeners can provide different types of responses to
 workload fluctuation conditions, such as: 
\end_layout

\begin_layout Itemize

\series bold
autoscaling
\series default
.
 automatically scaling IT resources out or in based on parameters previously
 defined by the cloud consumer
\end_layout

\begin_layout Itemize

\series bold
automatic notification
\series default
 of the cloud consumer when workloads exceed current thresholds or fall
 below allocated resources.
 This way, the cloud consumer can choose to adjust its current IT resource
 allocation
\end_layout

\begin_layout Standard
Different cloud provider vendors have different names for service agents
 that act as automated scaling listeners.
 
\end_layout

\begin_layout Subsection
Load Balancer
\end_layout

\begin_layout Standard
A common approach to horizontal scaling is to balance a workload across
 two or more IT resources to increase performance and capacity beyond what
 a single IT resource can provide.
 The 
\series bold
load balancer mechanism
\series default
 is a runtime agent with logic fundamentally based on this premise.
 Beyond simple division of labor algorithms, load balancers can perform
 a range of specialized runtime workload distribution functions that include:
 
\end_layout

\begin_layout Itemize

\series bold
asymmetric distribution
\series default
.
 Larger workloads are issued to IT resources with higher processing capacities
\end_layout

\begin_layout Itemize

\series bold
workload prioritization
\series default
.
 Workloads are scheduled, queued, discarded, and distributed workloads according
 to their priority levels
\end_layout

\begin_layout Itemize

\series bold
content-aware distribution
\series default
.
 Requests are distributed to different IT resources as dictated by the request
 content 
\end_layout

\begin_layout Standard
A load balancer is programmed or configured with a set of performance and
 QoS rules and parameters with the general objectives of optimizing IT resource
 usage, avoiding overloads, and maximizing throughput.
 The load balancer is typically located on the communication path between
 the IT resources generating the workload and the IT resources performing
 the workload processing.
\end_layout

\begin_layout Subsubsection
AWS Elastic Load Balancer
\end_layout

\begin_layout Standard
Elastic Load Balancing (
\series bold
ECB
\series default
) automatically distributes incoming traffic across multiple targets, such
 as EC2 instances, containers, and IP addresses, in one or more Availability
 Zones.
 It monitors the health of its registered targets, and routes traffic only
 to the healthy targets.
 A listener checks for connection requests from clients, using the protocol
 and port that you configure.
 The rules that you define for a listener determine how the load balancer
 routes requests to its registered targets.
 Each rule consists of a priority, one or more actions, and one or more
 conditions.
 ECB supports the following load balancers: Application Load Balancers,
 Network Load Balancers, Gateway Load Balancers, and Classic Load Balancers.
 
\end_layout

\begin_layout Standard
An 
\series bold
Application Load Balancer
\series default
, after it receives a request, evaluates the listener rules in priority
 order to determine which rule to apply, and then selects a target from
 the target group for the rule action.
 You can configure listener rules to route requests to different target
 groups based on the content of the application traffic.
 You can configure the routing algorithm used at the target group level.
 The default routing algorithm is round robin.
\end_layout

\begin_layout Standard
A 
\series bold
Network Load Balancer
\series default
 can handle millions of requests per second.
 After the load balancer receives a connection request, it selects a target
 from the target group for the default rule.
 It attempts to open a TCP connection to the selected target on the port
 specified in the listener configuration.
 
\end_layout

\begin_layout Subsection
Cloud usage monitor 
\end_layout

\begin_layout Standard
The cloud 
\series bold
usage monitor
\series default
 mechanism is a lightweight and autonomous software program responsible
 for collecting and processing IT resource usage data.
 Depending on the type of usage metrics they are designed to collect and
 the manner in which usage data needs to be collected, cloud usage monitors
 can exist in different formats.
 Each can be designed to forward collected usage data to a log database
 for post-processing and reporting purposes.
 
\end_layout

\begin_layout Subsubsection
Monitoring agent 
\end_layout

\begin_layout Standard
An intermediary, 
\series bold
event-driven 
\series default
program that exists as a service agent and resides along existing communication
 paths to transparently monitor and analyze dataflows.
 This type of cloud usage monitor is commonly used to measure network traffic
 and message metrics.
 
\end_layout

\begin_layout Subsubsection
Resource agent
\end_layout

\begin_layout Standard
A processing module that collects usage data by having 
\series bold
event-driven
\series default
 interactions with specialized resource software.
 This module is used to monitor usage metrics based on predefined, observable
 events at the resource software level, such as initiating, suspending,
 resuming, and vertical scaling.
 
\end_layout

\begin_layout Subsubsection
Polling agent
\end_layout

\begin_layout Standard
A processing module that collects cloud service usage data by 
\series bold
polling
\series default
 IT resources.
 This type of cloud service monitor is commonly used to periodically monitor
 IT resource status, such as uptime and downtime.
\end_layout

\begin_layout Subsubsection
Pay-per-use monitor
\end_layout

\begin_layout Standard
This mechanism measures cloud-based IT resource usage in accordance with
 predefined pricing parameters and generates usage logs for fee calculations
 and billing purposes.
 Some typical monitoring variables are request/response message quantity,
 transmitted data volume, and bandwidth consumption.
 The data collected by the 
\series bold
pay-per-use monitor
\series default
 is processed by a billing management system that calculates the payment
 fees.
\end_layout

\begin_layout Subsection
MAPE-K
\end_layout

\begin_layout Standard
The 
\series bold
MAPE-K
\series default
 (Monitor-Analyze-Plan-Execute over a shared Knowledge) is the most influential
 reference control model for autonomic and self-adaptive systems.
 A common approach to realize a feedback loop - a system for improving a
 product, process, etc.
 by collecting and reacting to events - is by means of a MAPE-K loop.
\end_layout

\begin_layout Standard
A 
\series bold
component Knowledge
\series default
 maintains data of the managed system and environment, adaptation goals,
 and other relevant states that are shared by the MAPE components.
 A 
\series bold
component Monitor
\series default
 gathers particular data from the underlying managed system and the environment
 through probes (or sensors) of the managed system, and saves data in the
 Knowledge.
 A 
\series bold
component Analyze
\series default
 performs data analysis to check whether an adaptation is required.
 If so, it triggers a 
\series bold
component Plan
\series default
 that composes a workflow of adaptation actions necessary to achieve the
 system’s goals.
 These actions are then carried out by a 
\series bold
component Execution
\series default
 through effectors (or actuators) of the managed system.
\end_layout

\begin_layout Standard
Computations M, A, P, and E may be made by multiple components that coordinate
 with one another to adapt the system when needed, i.e., they may be decentralized
 through multiple MAPE-K loops.
 These MAPE components can communicate explicitly or indirectly by sharing
 information in the knowledge repository.
\end_layout

\begin_layout Standard
AWS implements these componets through 
\series bold
CloudWatch
\series default
 (Monitor), 
\series bold
alarms
\series default
 (Analyze), 
\series bold
dynamic scaling
\series default
 (Plan), internal mechanisms or CLI commands (Execution), metrics (Knowedge).
\end_layout

\begin_layout Section
Kubernetes
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename kubernetes_components.png
	lyxscale 40
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kubernetes cluster with all its components tied together.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Kubernetes is a portable, extensible, open-source platform for managing
 containerized workloads and services, that facilitates both declarative
 configuration and automation.
 Kubernetes provides you with a framework to run distributed systems resiliently.
 lt takes care of scaling and failover for your application, provides deployment
 patterns, and more.
 Kubernetes provides you with:
\end_layout

\begin_layout Itemize

\series bold
service discovery
\series default
 and 
\series bold
load balancing
\series default
.
 If traffic to a container is high, Kubernetes is able to load balance and
 distribute the network traffic so that the deployment is stable
\end_layout

\begin_layout Itemize

\series bold
storage orchestration
\series default
.
 Kubernetes allows you to automatically mount a storage system of your choice,
 such as local storages, public cloud providers, and more
\end_layout

\begin_layout Itemize

\series bold
automated rollouts and rollbacks
\series default
.
 You can describe the desired state for your deployed containers using Kubernete
s, and it can change the actual state to the desired state at a controlled
 rate.
 
\end_layout

\begin_layout Itemize

\series bold
automatic bin packing
\series default
.
 You provide Kubernetes with a cluster of nodes that it can use to run container
ized tasks.
 You tell Kubernetes how much CPU and memory (RAM) each container needs.
 Kubernetes can fit containers onto your nodes to make the best use of your
 resources
\end_layout

\begin_layout Itemize

\series bold
self-healing
\series default
.
 Kubernetes restarts containers that fail, replaces containers, kills containers
 that do not respond to your user-defined health check, and does not advertise
 them to clients until they are ready to serve
\end_layout

\begin_layout Itemize

\series bold
secret and configuration management
\series default
.
 Kubernetes lets you stare and manage sensitive information, such as passwords,
 OAuth tokens, and SSH keys
\end_layout

\begin_layout Standard
Kubernetes operates at the container level rather than at the hardware level,
 providing some generally applicable features common to PaaS offerings,
 such as deployment, scaling, load balancing, and lets users integrate their
 logging, monitoring, and alerting solutions.
 However, Kubernetes is not monolithic, and these default solutions are
 optional and pluggable.
\end_layout

\begin_layout Subsection
Kubernetes components
\end_layout

\begin_layout Standard
When you deploy Kubernetes, you get a cluster.
 A 
\series bold
Kubernetes cluster
\series default
 consists of a set of worker machines, called 
\series bold
nodes
\series default
, that run containerized applications.
 Every cluster has at least one worker node.
 The worker node(s) host the 
\series bold
pods
\series default
, i.e.
 the components of the application workload.
 The 
\series bold
control plane
\series default
 manages the worker nodes and the pods in the cluster.
 In production environments, the control plane usually runs across multiple
 computers and a cluster usually runs multiple nodes, providing fault-tolerance
 and high availability.
\end_layout

\begin_layout Subsection
Control plane components 
\end_layout

\begin_layout Standard
The control plane components make global decisions about the cluster, e.g.
 scheduling, as well as detecting and responding to cluster events.
 Control plane components can be run on any machine in the cluster.
 However, for simplicity, setup scripts typically start all control plane
 components on the same machine, and do not run user containers on this
 machine.
 
\end_layout

\begin_layout Itemize

\series bold
kube-apiserver
\series default
.
 This is the fronted for the Kubernetes control plane, designed to scale
 horizontally - that is, it scales by deploying more instances - and balance
 traffic between those instances
\end_layout

\begin_layout Itemize

\series bold
kube-scheduler
\series default
.
 Watches for newly created pods with no assigned node, and selects a node
 for them to run on
\end_layout

\begin_layout Itemize

\series bold
kube-controller-manager
\series default
.
 Takes scaling decisions
\end_layout

\begin_layout Itemize
cloud-controller-manager.
 Combines several logically independent control loops into a single binary
 running as a single process, and links between cluster and cloud provider's
 API.
 It can scale horizontally to improve performance or to help tolerate failures
\end_layout

\begin_layout Subsection
Node Components 
\end_layout

\begin_layout Standard
Node components run on every node, maintaining running pods and providing
 the Kubernetes runtime environment.
 
\end_layout

\begin_layout Itemize
kube-proxy.
 Network proxy that runs on each node in the cluster.
 Maintains network rules on nodes and manages communications to pods from
 inside or outside of the cluster
\end_layout

\begin_layout Itemize
container runtime.
 Responsible for running containers.
 Kubernetes supports several container runtimes such as Docker, Containerd,
 and any implementation of the Kubernetes Container Runtime lnterface
\end_layout

\begin_layout Subsection
Addons: resource monitoring
\end_layout

\begin_layout Standard
Application performance in a Kubernetes cluster can be monitored by examining
 the containers, pods, services, and the characteristics of the overall
 cluster.
 Kubernetes provides detailed information about an application resource
 usage at each of these levels.
\end_layout

\begin_layout Section
Cloud storage
\end_layout

\begin_layout Standard
An ever increasing number of cloud-based services collect detailed data
 about their services and information about the users of these services.
 Then the service providers use the clouds to analyze that data.
 Storage and processing on the cloud are intimately tied to one another;
 indeed, sophisticated strategies to reduce the access time and to support
 real-time multimedia access are necessary to satisfy the requirements of
 content delivery.
 On the other hand, most cloud applications process very large amounts of
 data; effective data replication and storage management strategies are
 critical to the computations performed on the cloud.
 
\end_layout

\begin_layout Subsection
Atomic actions
\end_layout

\begin_layout Standard
Parallel and distributed applications must take special precautions for
 handling shared resources.
 In many cases, a multistep operation should be allowed to proceed to completion
 without any interruptions, and the operation should be 
\series bold
atomic
\series default
.
 The instruction sets of most processors support the 
\series bold
test-and-set
\series default
 instruction, which writes to a memory location and returns the old content
 of that memory cell as non-interruptible operations.
 Other architectures support 
\series bold
compare-and-swap
\series default
, an atomic instruction that compares the contents of a memory location
 to a given value and, only if the two values are the same, modifies the
 contents of that memory location to a given new value.
 
\end_layout

\begin_layout Standard
Two flavors of atomicity can be distinguished: all-or-nothing and before-or-afte
r atomicity.
 
\series bold
All-or-nothing
\series default
 means that either the entire atomic action is carried out, or the system
 is left in the same state it was before the atomic action was attempted.
 
\series bold
Before-or-after
\series default
 atomicity means that, from the point of view of an external observer, the
 effect of multiple actions is as though these actions have occurred one
 after another, in some order.
 A stronger condition is to impose a sequential order among transitions.
\end_layout

\begin_layout Subsubsection
Storage models, filesystems, and databases
\end_layout

\begin_layout Standard
A 
\series bold
storage model
\series default
 describes the layout of a data structure in physical storage; a data model
 captures the most important logical aspects of a data structure in a database.
 Two abstract models of storage are commonly used: cell storage and journal
 storage.
 
\series bold
Cell storage
\series default
 assumes that the storage consists of cells of the same size and that each
 object fits exactly in one cell.
 Once the content of a cell is changed by an action, there is no way to
 abort the action and restore the original content of the cell.
 To be able to restore a previous value we have to maintain a version history
 for each variable in the cell storage.
\end_layout

\begin_layout Standard

\series bold
Journal storage
\series default
 consists of a manager and cell storage, where the entire history of a variable
 is maintained, rather than just the current value.
 The user does not have direct access to the cell storage; instead the user
 can request the journal manager to (i) start a new 
\series bold
action
\series default
; (ii) read the value of a cell; (iii) write the value of a cell; (iv) 
\series bold
commit
\series default
 an action; or (v) 
\series bold
abort
\series default
 an action.
 An all-or-nothing action first records the action in a log in journal storage
 and then installs the change in the cell storage by overwriting the previous
 version of a data item.
\end_layout

\begin_layout Standard
Many cloud applications must support online transaction processing and have
 to guarantee the correctness of the transactions.
 Transactions consist of multiple actions and the system may fail during
 or after each one of the actions, and steps to ensure correctness must
 be taken.
 Correctness of a transaction means that the result should be guaranteed
 to be the same as though the actions were applied one after another, regardless
 of the order.
 To guarantee correctness, a transaction-processing system supports all-or-nothi
ng atomicity.
\end_layout

\begin_layout Subsection
Google File System 
\end_layout

\begin_layout Standard
The 
\series bold
GFS
\series default
 was designed after a careful analysis of the file characteristics and of
 the access models.
 First, components failure are the norm rather than the exception.
 The file system consists of hundred or even thousand of commodity storage
 machines, accessed by a comparable number of clients.
 Therefore, scalability and reliability are critical features of the system.
 Second, files size range from a few GB to hundreds of TB.
 Third, random writes are extremely infrequent, and the most common operation
 is to append to an existing file.
 Once written, files are only read, and often only sequentially.
 Finally, users often process data in bulk and are less concerned with the
 response time, and the consistency model should be relaxed to simplify
 the system implementation.
\end_layout

\begin_layout Subsubsection
Architecture
\end_layout

\begin_layout Standard
A GFS cluster consists of a single 
\series bold
master
\series default
 and multiple 
\series bold
chunkServers
\series default
, and is accessed by multiple clients.
 Files are divided into fixed-size 
\begin_inset Formula $64\text{-MB}$
\end_inset

 
\series bold
chunks
\series default
 and stored on chunkServers' local disks as Linux files.
 For reliability, each chunk is replicated by default on three chunkServers.
 A large chunk size reduces clients' interactions with the master, and can
 reduce network overhead, since a client is more likely to perform many
 operations on the same chunk.
\end_layout

\begin_layout Standard
The master maintains all file system metadata such as the namespace, access
 control informations, mapping from files to chunks, and current locations
 of chunks.
 Neither clients nor chunkServers cache file data, since most clients' applicati
on stream through huge files or have datasets too large to be cached.
 However, clients do cache metadata.
\end_layout

\begin_layout Standard
Having a single master vastly simplifies the overal system design, but its
 involvement must be minimized in order to avoid possible bottlenecks.
 Files creation is handled directly by the master, while clients never read
 and write file data through the master.
 Instead, clients ask the master which chunkServer it should contact and
 caches this information.
\end_layout

\begin_layout Standard
The master does not keep a persistent record of which chunkServers have
 a replica of a given chunk.
 It simply polls chunkServers for that information at startup.
 Furthermore, the master keeps in stable memory an operational log of critical
 metadata changes.
 Given its nature, log changes are not visible to clients until metadata
 changes are made persistent.
 The master recovers its file system state by replaying the operation log,
 and checkpoints its state whenever the log grows beyond a certain size.
\end_layout

\begin_layout Subsection
Apache Hadoop filesystem
\end_layout

\begin_layout Standard
Hadoop provides a distributed file system and a framework for the analysis
 and transformation of very large datasets using the MapReduce paradigm.
 HDFS stores metadata on a dedicated server, called the 
\series bold
NameNode
\series default
.
 Application data are stored on other servers called 
\series bold
DataNodes
\series default
.
\end_layout

\begin_layout Standard
File content is split into large blocks, typically 
\begin_inset Formula $128\text{-MB}$
\end_inset

, and each block is independently replicated at multiple DataNodes for reliabili
ty.
 The NameNode maintains the namespace tree and the mapping of file blocks
 to DataNodes.
\end_layout

\begin_layout Subsubsection
File I/O operations
\end_layout

\begin_layout Standard
An application adds data to HDFS by creating a new file and writing the
 data to it.
 After the file is closed, the bytes written cannot be altered or removed
 except that new data can be added to the file by reopening the file for
 append.
 HDFS implements a single-writer, multiple-reader model.
 The design of HDFS I/O is particularly optimized for batch processing systems,
 like MapReduce, which require high throughput for sequential reads and
 writes.
\end_layout

\begin_layout Standard
An HDFS file consists of blocks.
 When there is a need for a new block, the NameNode allocates a block and
 determines a list of dataNodes to host replicas of the block.
 The dataNodes form a 
\series bold
pipeline
\series default
, the order of which minimizes the total network distance from the client
 to the last DataNode.
 Bytes are pushed to the pipeline as a sequence of packets.
 The client is then reponsible for sending an acknowledgement message to
 the NameNode.
\end_layout

\begin_layout Standard
When a client opens a file to read, it fetches the list of blocks and the
 locations of each block replica from the NameNode.
 The locations of each block are ordered by their distance from the reader.
 A read may fail if the target DataNode is unavailable, the node no longer
 hosts a replica of the block, or the replica is found to be corrupt when
 checksums are tested.
 
\end_layout

\begin_layout Subsection
BigTable 
\end_layout

\begin_layout Standard

\series bold
Bigtable
\series default
 is built upon GFS to store log and data files, it employes the Google SSTable
 file format to store internal data, and relies on a highly-available and
 persistent distributed lock service called Chubby.
 A Bigtable is a sparse, distributed, persistent multi-dimensional sorted
 map.
 The map is indexed by a row key, column key, and a timestamp.
 
\begin_inset Formula 
\[
(\text{row:}string,\text{column:}string,\text{time:}int64)\rightarrow string
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Indexes
\end_layout

\begin_layout Standard
The 
\series bold
row keys
\series default
 in a table are arbitrary strings and every read or write of data under
 a single row key is atomic, a design decision that makes it easier for
 clients to reason about the system's behavior in the presence of concurrent
 updates to the same row.
 The row range for a table is dynamically partitioned.
 Each row range is called a 
\series bold
tablet
\series default
, which is the unit of distribution and load balancing.
\end_layout

\begin_layout Standard

\series bold
Column keys
\series default
 are grouped into sets called column families, which form the basic unit
 of access control.
 All data stored in a column family is usually of the same type and compressed
 together.
 A column family must be created before data can be stored under any column
 key in that family.
 The number of distinct column families in a table is kept small, and families
 rarely change during operation.
 In contrast, a table may have an unbounded number of columns.
\end_layout

\begin_layout Standard
Each cell in a Bigtable can contain multiple versions of the same data;
 these versions are indexed by 
\series bold
timestamp
\series default
.
 Different versions of a cell are stored in decreasing timestamp order,
 so that the most recent versions can be read first.
 To make the management of versioned data less onerous, Bigtable garbage-collect
s cell versions automatically.
 The client can specify either that only the last 
\begin_inset Formula $n$
\end_inset

 versions of a cell be kept, or that only new-enough versions be kept.
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Standard
A Bigtable cluster stores a number of tables.
 Each table consists of a set of tablets, and each tablet contains all data
 associated with a row range.
 Initially, each table consists of just one tablet.
 As a table grows, it is automatically split into multiple tablets.
\end_layout

\begin_layout Standard
The 
\series bold
master
\series default
 is responsible for assigning tablets to TabletServers, balancing tablet-server
 load, and garbage collection of files in GFS.
 In addition, it handles schema changes such as table and column family
 creations.
 Each 
\series bold
tabletServer
\series default
 manages a set of tablets, and can be dynamically added (or removed) from
 a cluster to accomodate changes in workloads.
 A tabletServer handles read and write requests to the tablets that it has
 loaded, and also splits tablets that have grown too large.
 
\end_layout

\begin_layout Standard
As with many single-master distributed storage systems, client data does
 not move through the master: clients communicate directly with tabletServers
 for reads and writes.
 Because Bigtable clients do not rely on the master for tablet location
 information, most clients never communicate with the master.
 
\end_layout

\begin_layout Subsection
Transaction processing and NoSQL databases 
\end_layout

\begin_layout Standard
Many cloud services are based on online transaction processing (
\series bold
OLTP
\series default
) and operate under tight latency constraints.
 Moreover, these applications have to deal with extremely high data volumes
 and are expected to provide reliable services for very large communities
 of users.
 A major concern for the designers of OLTP systems is to reduce the 
\series bold
response time
\series default
.
 
\series bold
Scalability
\series default
 is the other major concern for cloud OLTP applications and implicitly for
 datastores.
 There is a distinction between 
\series bold
vertical scaling
\series default
, where the data and the workload are distributed to systems that share
 resources such as cores and processors, disks, and possibly RAM, and 
\series bold
horizontal scaling
\series default
, where the systems do not share either primary or secondary storage
\end_layout

\begin_layout Standard
The “soft-state” approach in the design of 
\series bold
NoSQL
\series default
 allows data to be inconsistent and ensures that data will be “eventually
 consistent” at some future point in time instead of enforcing consistency
 at the time when a transaction is committed.
 Data partitioning among multiple storage servers and data replication are
 also tenets of the NoSQL philosophy; they increase availability, reduce
 response time, and enhance scalability.
 
\end_layout

\begin_layout Subsection
Amazon Dynamo
\end_layout

\begin_layout Standard

\series bold
Dynamo
\series default
 is used to manage the state of services that have very high reliability
 requirements and need tight control over the tradeoffs between availability,
 consistency, cost-effectiveness and performance.
 There are many services on Amazon's platform that only need primary-key
 access to a data store.
 Dynamo uses a synthesis of well known techniques to achieve scalability
 and availability: Data is partitioned and replicated using consistent hashing,
 and consistency is facilitated by object versioning.
 The consistency among replicas during updates is maintained by a quorum-like
 technique and a decentralized replica synchronization protocol.
\end_layout

\begin_layout Itemize

\series bold
query model
\series default
.
 Simple read and write operations to a data item that is uniquely identified
 by a key.
 No operations span multiple data items and there is no need for relational
 schema
\end_layout

\begin_layout Itemize

\series bold
ACID properties
\series default
.
 Experience at Amazon has shown that data stores that provide ACID guarantees
 tend to have poor availability.
 Dynamo targets applications that operate with weaker consistency (the “C”
 in ACID) if this results in high availability
\end_layout

\begin_layout Itemize

\series bold
efficiency
\series default
 and other assumptions.
 The system needs to function on a commodity hardware infrastructure.
 The tradeoffs are in performance, cost efficiency, availability, and durability
 guarantees.
 Furthermore, Dynamo operation environment is assumed to be non-hostile
 and there are no security related requirements such as authentication and
 authorization
\end_layout

\begin_layout Subsubsection
Design considerations
\end_layout

\begin_layout Standard
Dynamo is designed to be an eventually consistent data store; that is all
 updates reach all replicas eventually.
 An important design consideration is to decide when to perform the process
 of resolving update conflicts, i.e., whether conflicts should be resolved
 during reads or writes.
 Dynamo targets the design space of an “always writeable” data store, forcing
 the complexity of conflict resolution to the reads in order to ensure that
 writes are never rejected.
\end_layout

\begin_layout Standard
The next design choice is who performs the process of conflict resolution.
 This can be done by the data store or the application.
 If conflict resolution is done by the data store, its choices are rather
 limited.
 In such cases, the data store can only use simple policies, such as “last
 write wins”, to resolve conflicting updates.
 On the other hand, since the application is aware of the data schema it
 can decide on the conflict resolution method that is best suited for its
 client's experience.
 Other considerations:
\end_layout

\begin_layout Itemize

\series bold
incremental scalability
\series default
.
 Dynamo should be able to scale out one storage host at a time, with minimal
 impact on both operators of the system and the system itself
\end_layout

\begin_layout Itemize

\series bold
symmetry
\series default
.
 Every node in Dynamo should have the same set of responsibilities as its
 peers
\end_layout

\begin_layout Itemize

\series bold
decentralization
\series default
.
 The design should favor decentralized peer-to-peer techniques over centralized
 control
\end_layout

\begin_layout Itemize

\series bold
heterogeneity
\series default
.
 The system needs to be able to exploit heterogeneity in the infrastructure
 it runs on.
 e.g.
 the work distribution must be proportional to the capabilities of the individua
l servers
\end_layout

\begin_layout Standard
Dynamo stores objects associated with a key through a simple interface;
 it exposes two operations: 
\begin_inset Formula $get(\,)$
\end_inset

 and 
\begin_inset Formula $put(\,)$
\end_inset

.
 The 
\begin_inset Formula $get(key)$
\end_inset

 operation locates the object replicas associated with the key in the storage
 system and returns a single object or a list of objects with conflicting
 versions along with a context.
 The 
\begin_inset Formula $put(key,context,object)$
\end_inset

 operation determines where the replicas of the object should be placed
 based on the associated key, and writes the replicas to disk.
 The context encodes system metadata about the object that is opaque to
 the caller and includes information such as the version of the object.
 The context information is stored along with the object so that the system
 can verify the validity of the context object supplied in the put request.
\end_layout

\begin_layout Subsubsection
Partitioning algorithm
\end_layout

\begin_layout Standard
One of the key design requirements for Dynamo is that it must scale incrementall
y.
 This requires a mechanism to dynamically partition the data over the set
 of nodes in the system.
 Dynamo's partitioning scheme relies on consistent hashing to distribute
 the load across multiple storage hosts.
 In consistent hashing, the output range of a hash function is treated as
 a fixed circular space or “ring”.
 Each node in the system is assigned a random value within this space which
 represents its “position” on the ring.
 Each data item identified by a key is assigned to a node by hashing the
 data item’s key to yield its position on the ring, and then walking the
 ring clockwise to find the first node with a position larger than the item's
 position.
\end_layout

\begin_layout Standard
The basic consistent hashing algorithm presents some challenges.
 First, the random position assignment of each node on the ring leads to
 non-uniform data and load distribution.
 Second, the basic algorithm is oblivious to the heterogeneity in the performanc
e of nodes.
 To address these issues, Dynamo uses a variant of consistent hashing: instead
 of mapping a node to a single point in the circle, each node gets assigned
 to multiple points in the ring.
 To this end, Dynamo uses the concept of “virtual nodes”.
 A virtual node looks like a single node in the system, but each node can
 be responsible for more than one virtual node.
 Effectively, when a new node is added to the system, it is assigned multiple
 positions (henceforth, “tokens”) in the ring.
\end_layout

\begin_layout Subsubsection
Replication
\end_layout

\begin_layout Standard
To achieve high availability and durability, Dynamo replicates its data
 on multiple hosts.
 Each data item is replicated at 
\begin_inset Formula $N$
\end_inset

 hosts, where 
\begin_inset Formula $N$
\end_inset

 is a parameter configured per-instance.
 Each key, 
\begin_inset Formula $k$
\end_inset

, is assigned to a coordinator node.
 The coordinator is in charge of the replication of the data items that
 fall within its range.
 In addition to locally storing each key within its range, the coordinator
 replicates these keys at the 
\begin_inset Formula $N-1$
\end_inset

 clockwise successor nodes in the ring.
 This results in a system where each node is responsible for the region
 of the ring between it and its 
\begin_inset Formula $N_{th}$
\end_inset

 predecessor.
\end_layout

\begin_layout Subsubsection
Data versioning
\end_layout

\begin_layout Standard
Dynamo provides eventual consistency, which allows for updates to be propagated
 to all replicas asynchronously.
 A 
\begin_inset Formula $put(\,)$
\end_inset

 call may return to its caller before the update has been applied at all
 the replicas, which can result in scenarios where a subsequent 
\begin_inset Formula $get(\,)$
\end_inset

 operation may return an object that does not have the latest updates.
 There is a category of applications in Amazon's platform that can tolerate
 such inconsistencies and can be constructed to operate under these conditions.
\end_layout

\begin_layout Standard
In order to provide this kind of guarantee, Dynamo treats the result of
 each modification as a new and immutable version of the data.
 It allows for multiple versions of an object to be present in the system
 at the same time.
 Most of the time, new versions subsume the previous version(s), and the
 system itself can determine the authoritative version.
 Dynamo uses vector clocks in order to capture causality between different
 versions of the same object.
\end_layout

\begin_layout Section
Big data
\end_layout

\begin_layout Subsection
Google MapReduce
\end_layout

\begin_layout Standard

\series bold
MapReduce
\series default
 is based on a very simple idea for parallel processing of data-intensive
 applications.
 First, split the data into blocks, assign each block to an instance or
 process, and run these instances in parallel.
 Once all the instances have finished, the computations assigned to them
 start the second phase: Merge the partial results produced by individual
 instances.
\end_layout

\begin_layout Standard
The MapReduce is a programming model conceived for processing and generating
 large data sets on computing clusters.
 As a result of the computation, a set of input 
\begin_inset Formula $\left\langle \text{key},\text{value}\right\rangle $
\end_inset

 pairs is transformed into a set of output 
\begin_inset Formula $\left\langle \text{key}',\text{value}'\right\rangle $
\end_inset

 pairs.
 Call 
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 the number of Map and Reduce tasks, respectively, and 
\begin_inset Formula $N$
\end_inset

 the number of systems used by the MapReduce.
 When a user program invokes the MapReduce function, the following sequence
 of actions take place:
\end_layout

\begin_layout Enumerate
the MapReduce library first splits the input files into 
\begin_inset Formula $M$
\end_inset

 pieces of typically 
\begin_inset Formula $16\text{-MB}$
\end_inset

 to 
\begin_inset Formula $64\text{-MB}$
\end_inset

 per piece.
 It then starts up many copies of the program on a cluster of machines.
 One of the copies of the program is the master, which picks idle workers
 and assigns each one a map task or a reduce task
\end_layout

\begin_layout Enumerate
a worker who is assigned a map task parses 
\begin_inset Formula $\left\langle \text{key},\text{value}\right\rangle $
\end_inset

 pairs out of the input data and passes each pair to the user-defined Map
 function.
 The intermediate 
\begin_inset Formula $\left\langle \text{key},\text{value}\right\rangle $
\end_inset

 pairs produced by the Map function are buffered in memory
\end_layout

\begin_layout Enumerate
periodically, the buffered pairs are written to local disk and partitioned
 into 
\begin_inset Formula $R$
\end_inset

 regions.
 The locations of these buffered pairs on the local disk are passed back
 to the master, who is responsible for forwarding these locations to the
 reduce workers
\end_layout

\begin_layout Enumerate
when a reduce worker has read all intermediate data, it sorts it by the
 intermediate keys so that all occurrences of the same key are grouped together.
 The sorting is needed because typically many different keys map to the
 same reduce task.
 The reduce worker iterates over the sorted intermediate data and for each
 unique intermediate key encountered, it passes the key and the corresponding
 set of intermediate values to the user's Reduce function.
 The output of the Reduce function is appended to a final output file for
 this reduce partition
\end_layout

\begin_layout Subsubsection
Fault tolerance
\end_layout

\begin_layout Standard
Any map tasks completed by failed worker are reset back to their initial
 idle state, and therefore become eligible for scheduling on other workers.
 Similarly, any map task or reduce task in progress on a failed worker is
 also reset to idle and becomes eligible for rescheduling.
 Completed map tasks are re-executed on a failure because their output is
 stored on the local disks of the failed machine and is therefore inaccessible.
 Completed reduce tasks do not need to be re-executed since their output
 is stored in a global file system.
 
\end_layout

\begin_layout Standard
Given that there is only a single Master, its failure is unlikely; therefore
 standard implementations abort the MapReduce computation if the Master
 fails.
 Clients can check for this condition and retry the MapReduce operation
 if they desire.
 
\end_layout

\begin_layout Subsection
Apache Hadoop MapReduce
\end_layout

\begin_layout Standard
MapReduce constitutes a simplified model for processing large quantities
 of data and imposes constraints on the way distributed algorithms should
 be organized to run over a MapReduce infrastructure.
 Although the model can be applied to several different problem scenarios,
 it still exhibits limitations, mostly due to the fact that the abstractions
 provided to process data are very simple, and complex problems might require
 considerable effort to be represented in terms of map and reduce functions
 only.
 Therefore, a series of extensions to and variations of the original MapReduce
 model have been proposed.
\end_layout

\begin_layout Subsubsection
Shuffle and sort
\end_layout

\begin_layout Standard
MapReduce makes the guarantee that the input to every reducer is sorted
 by key.
 The process by which the system performs the sort - and transfers the map
 outputs to the reducers as inputs - is known as the 
\series bold
shuffle
\series default
.
 When the map function starts producing output, before it writes to disk,
 the thread first divides the data into partitions corresponding to the
 reducers that they will ultimately be sent to.
 Within each partition, the background thread performs an in-memory sort
 by key.
\end_layout

\begin_layout Standard
The map tasks may finish at different times, so the reduce task starts copying
 their outputs as soon as each completes.
 This is known as the copy phase of the reduce task.
 When all the map outputs have been copied, the reduce task moves into the
 
\series bold
sort
\series default
 phase, which merges the map outputs, maintaining their sort ordering.
 During the reduce phase, the reduce function is invoked for each key in
 the sorted output.
 The output of this phase is written directly to the output filesystem,
 typically HDFS.
\end_layout

\begin_layout Subsubsection
Grid computing
\end_layout

\begin_layout Standard
The approach in high-performance computing is to distribute the work across
 a cluster of machines, which access a shared filesystem.
 This works well for predominantly compute-intensive jobs, but it becomes
 a problem when nodes need to access larger data volumes, since the network
 bandwidth is the bottleneck and compute nodes become idle.
 Hadoop tries to co-locate the data with the compute nodes, so data access
 is fast because it is local.
\end_layout

\begin_layout Standard
This feature, known as 
\series bold
data locality
\series default
, is at the heart of data processing in Hadoop and is the reason for its
 good performance.
 Recognizing that network bandwidth is the most precious resource in a data
 center environment (it is easy to saturate network links by copying data
 around), Hadoop goes to great lengths to conserve it by explicitly modeling
 network topology.
 If for some reason data locality is not possible, Hadoop mapReduce fallback
 to, in order, rack-local or off-rack deployment/execution.
\end_layout

\begin_layout Subsection
Apache Hadoop YARN
\end_layout

\begin_layout Standard
The fundamental idea of YARN is to split up the functionalities of resource
 management and job scheduling/monitoring into separate daemons.
 An application is either a single job or a DAG of jobs.
\end_layout

\begin_layout Standard
The ResourceManager and the NodeManager form the data-computation framework.
 The 
\series bold
ResourceManager
\series default
 is the ultimate authority that arbitrates resources among all the applications
 in the system.
 The 
\series bold
NodeManager
\series default
 is the per-machine framework agent who is responsible for containers, monitorin
g their resource usage (cpu, memory, disk, network) and reporting the same
 to the ResourceManager/Scheduler.
\end_layout

\begin_layout Standard
The ResourceManager has two main components: Scheduler and ApplicationsManager.
 The 
\series bold
Scheduler
\series default
 is responsible for allocating resources to the various running applications.
 The Scheduler is pure scheduler in the sense that it performs no monitoring
 or tracking of status for the application.
 Also, it offers no guarantees about restarting failed tasks either due
 to application failure or hardware failures.
 The Scheduler performs its scheduling function based on the resource requiremen
ts of the applications; it does so based on the abstract notion of a resource
 Container which incorporates elements such as memory, cpu, disk, network
 etc.
\end_layout

\begin_layout Standard
The 
\series bold
ApplicationsManager
\series default
 is responsible for accepting job-submissions, negotiating the first container
 for executing the application specific ApplicationMaster and provides the
 service for restarting the ApplicationMaster container on failure.
 Furthermore, it negotiates appropriate resource containers from the Scheduler,
 tracking their status and monitoring for progress.
 The ApplicationMaster works with the NodeManager(s) to execute and monitor
 the tasks.
\end_layout

\begin_layout Standard
YARN supports the notion of resource reservation via the 
\series bold
ReservationSystem
\series default
, a component that allows users to specify a profile of resources over-time
 and temporal constraints, e.g.
 deadlines, and reserve resources to ensure the predictable execution of
 important jobs.
 The ReservationSystem tracks resources over-time, performs admission control
 for reservations, and dynamically instruct the underlying scheduler to
 ensure that the reservation is fullfilled.
\end_layout

\begin_layout Standard
In order to scale YARN beyond few thousands nodes, YARN supports the notion
 of 
\series bold
Federation
\series default
: it allows to transparently wire together multiple YARN (sub-)clusters,
 and make them appear as a single massive cluster.
 This can be used to achieve larger scale, and/or to allow multiple independent
 clusters to be used together for very large jobs.
\end_layout

\begin_layout Subsection
Apache Spark
\end_layout

\begin_layout Standard
Models of computing like MapReduce allow to execute data-parallel computations
 on clusters of unreliable machines, by systems that automatically provide
 locality-aware scheduling, fault tolerance, and load balancing.
 While this data flow programming model is useful for a large class of applicati
ons, others cannot be expressed efficiently, i.e.
 those that reuse a working set of data across multiple parallel operations.
 This includes two use cases such as iterative jobs - common in ML algorithms,
 where a function is applied repeatedly on the same dataset - and interactive
 analytics - run ad-hoc exploratory queries on large datasets, through SQL
 interfaces.
\end_layout

\begin_layout Standard

\series bold
Spark
\series default
 has a programming model similar to MapReduce but extends it with a data-sharing
 abstraction called Resilient Distributed Datasets (
\series bold
RDD
\series default
), and support for parallel operations on these datasets.
 Spark is designed to support multiple external systems for persistent storage.
\end_layout

\begin_layout Subsubsection
Cluster-mode architecture
\end_layout

\begin_layout Standard
Spark applications run as independent sets of processes on a cluster, coordinate
d by the 
\series bold
SparkContext
\series default
 object in the main program, called 
\series bold
Driver
\series default
.
 Specifically, to run on a cluster, the SparkContext can connect to several
 types of cluster managers, e.g.
 YARN, Mesos, Kubernetes, which allocate resources across applications.
 Once connected, Spark acquires 
\series bold
executors
\series default
 on nodes in the cluster, which are processes that run computations and
 store data for your application.
 Next, it sends application code to the executors.
 Finally, SparkContext sends tasks to the executors to run.
\end_layout

\begin_layout Standard
Applications are isolated in term of scheduling and executors, since tasks
 from different applications runs on different JVMs, but this also means
 that no sharing of data across different applications is possible.
\end_layout

\begin_layout Subsubsection
Resilient Distributed Datasets
\end_layout

\begin_layout Standard
A RDD is a read-only collection of objects partitioned across a set of machines
 that can be rebuilt if a partition is lost.
 Spark lets programmers construct RDDs in different ways:
\end_layout

\begin_layout Itemize
from a file in a shared file system, such as HDFS
\end_layout

\begin_layout Itemize
by “parallelizing” a collection in the driver program, which means dividing
 it into a number of slices that will be sent to multiple nodes
\end_layout

\begin_layout Itemize
by transforming an existing RDD
\end_layout

\begin_layout Standard
Spark evaluates RDDs lazily, allowing it to find an efficient plan for the
 user's computation.
 When an action is called, Spark looks at the whole graph of transformations
 used to create an execution plan.
 Finally, RDDs provide explicit support for data sharing among computations,
 by avoiding replication of intermediate data.
 RDDs are “ephemeral” by default, in that they get recomputed each time
 they are used in an action.
 However, users can also persist selected RDDs in storage memory or cache
 them in main memory.
\end_layout

\begin_layout Standard
RDDs also automatically recover from failures.
 Traditionally, distributed computing systems have provided fault tolerance
 through data replication or checkpointing.
 Spark uses a different approach called 
\series bold
lineage
\series default
.
 Each RDD tracks the graph of transformations that was used to build it
 and reruns these operations on base data to reconstruct any lost partitions.
 Lineage-based recovery is significantly more efficient than replication
 in data-intensive workloads.
\end_layout

\begin_layout Standard
RDDs are best suited for batch applications that apply the same operation
 to all elements of a dataset.
 In these cases, RDDs can efficiently remember each transformation as one
 step in a lineage graph and can recover lost partitions without having
 to log large amounts of data.
 RDDs would be less suitable for applications that make asynchronous finegrained
 updates to shared state, such as a storage system for a web application
 or an incremental web crawler.
 For these applications, it is more efficient to use systems that perform
 traditional update logging and data checkpointing.
\end_layout

\begin_layout Standard
RDDs support two kind of operations, transformations - lazy operations that
 define a new RDD, e.g.
 
\begin_inset Formula $map(\,)$
\end_inset

, 
\begin_inset Formula $filter(\,)$
\end_inset

, 
\begin_inset Formula $reduceByKey(\,)$
\end_inset

, and 
\begin_inset Formula $sort(\,)$
\end_inset

 - and actions - execute a computation to return a value to the program
 or write data to external storage, e.g.
 
\begin_inset Formula $count(\,)$
\end_inset

, 
\begin_inset Formula $collect(\,)$
\end_inset

, 
\begin_inset Formula $reduce(\,)$
\end_inset

, and 
\begin_inset Formula $save(\,)$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Higher-level libraries
\end_layout

\begin_layout Standard
The RDD programming model provides only distributed collections of objects
 and functions to run on them.
 Using RDDs, Spark developers have built a variety of higher-level libraries,
 targeting many of the use cases of specialized computing engines.
\end_layout

\begin_layout Standard

\series bold
Spark SQL
\series default
 supports relational queries on Spark, using techniques similar to analytical
 databases (compressed columnar storage).
 Furthermore, Spark provides a higher-level abstraction for basic data transform
ations called 
\series bold
DataFrames
\series default
, which are RDDs of records with a known schema.

\series bold
 Spark Streaming
\series default
 implements incremental stream processing using a model called “discretized
 streams
\begin_inset Quotes erd
\end_inset

.
 To implement streaming over Spark, input data is split into small batches,
 which is regularly combine with state stored inside RDDs to produce new
 results.
 
\series bold
GraphX
\series default
 provides a graph computation interface.

\series bold
 MLlib
\series default
 for distributed machine learning model training.
\end_layout

\end_body
\end_document
